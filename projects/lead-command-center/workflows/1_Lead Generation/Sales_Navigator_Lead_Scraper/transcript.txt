if you are still manually searching LinkedIn for leads you're wasting hours maybe even days on something that can be fully automated imagine pulling thousands of Target leads emails included on autopilot with zero manual effort no more copying and pasting no more struggling to find contact details just high quality leads delivered straight to you today I'm going to show you exactly how to do it sales Navigator is one of the best tools out there for finding how high quality leads but most people use it the hard way they spend

hours applying filters manually coping profiles into spreadsheets and then worst of all hunting down emails one by one it is slow frustrating and by the time you're done the best leads are already working with someone else if you're new here my name is Clarence I build automated workflows in NN to radically improve lead Generations I use these systems in my own business I use them for my clients and today I'm giving you the exact blueprint so you can start using them too the workflow I'll be showcasing today will extract leads

directly from sales Navigator complete with emails and all the data you need so you can focus on closing deals instead of chasing leads with a simple workflow built in n8n we're going to one find high quality leads using LinkedIn sales Navigator we're going to extract their full contact details with an apify scraper and we're going to automate the entire process so you can get leads on autopilot once this is set up all you have to do is press a button and watch verified leads come into your CRM

automatically now today's focus is purely on extraction so getting the leads but once you have them you can enrich their profiles with even more data and use AI power personalization to craft Outreach emails that actually get responses I'll briefly show you how that works too so you don't just have leads you have warm prospects ready for cont the best part is you don't have to build this from scratch or have to copy me in this video because I just launched a brand new school Community or you can

download this exact workflow and access every automation I've shared so far plus exclusive systems I haven't shown anywhere else I'm constantly adding new Blueprints and automation strategies so if you're serious about scaling your lead generation this is where you need to be link is in the description now let's jump in and start building the system so as I said the focus is going to be on extracting leads from LinkedIn sales Navigator I'll touch upon the enrichment part and the AI personalization part later on in this

video so let me quickly zoom in so you guys can get a glimpse of what I'm talking about okay so from left to right as always in order to extract any information from LinkedIn sales Navigator we'll be needing to use a scraper we're using an a API scraper to be more precise I've pasted the URL to the apy scraper here I'll quickly go over it so you know exactly what you need to set this up and connect this to your own workflow so what you see on screen is the apify scraper it's called LinkedIn sales Navigator scraper so it's

very straightforward to connect a scraper to your NN workflow you're going to be needing three things you're going to be needing your API Keys which are personal and can be found in your profile settings you're going to be needing the URL endpoint which can be found in the API documentation of apify but I'll quickly show you because it's already set up inside of the HTTP node and you'll be needing the Json body which includes the parameters which are needed for the scraper to start running as I said the API keys are inside of

your personal profile inside of these settings and endpoint URL is not on this page but it's just a generic one all you need is the scraper ID which is inside of URL I know it's just out cropped out of your screen but I'll go over it inside of your note in a second first let me show you how to set up this scraper and get the Json code which you need to copy and paste later on so to set this up first you'll be needing your LinkedIn cookie your LinkedIn cookie is essentially just stored data of your

LinkedIn profile which gives the API the right credentials to start using your account account to scrape the information that may have sounded a bit more difficult than it needs to be all you need to do is download a Chrome extension the cookie editor if you press on the question mark you can just press on the link and download it yourself then you would need to go on to your sales Navigator account go to the Chrome extension cookie editor just export as Json go back to the apy and paste it in the cookie section it would look

something like this this is going to be different for each and every one of us because we have different login credentials but it is very straightforward to set up as I said all you need is a Chrome extension and just export it manually paste it in here and you're done forever then if we scroll down a bit you can see that it requires us to input a search URL you can easily get the URL by going to LinkedIn sales Navigator before you straight up copy and paste your url you might want to narrow down your search you can do that

by using the filters inside of LinkedIn sales Navigator they make it very easy for you to kind of pinpoint your ideal customer and create a very targeted lead list so let me walk you through some of the options if you have a specific company in mind you want to Target you can do this under the company section I wouldn't really recommend filtering it down Company by company but one of the filters I would recommend using is the company headcount one this allows you to kind of filter for companies with a specific headcount

so let's say say you're just like me you're offering lead generation services so you're basically replacing an SDR in that case you don't want to Target companies which already have an extensive sales department because there's a very slim chance they're going to be needing your services so in my instance I would for example only target businesses which have a headcount between 1 and 10 or maybe 11 and 50 if I'm feeling frisky moving on you can also uh select or filter by company headquarters location might be

interesting let's say your only offering service in California for example then this is one you want to fill out now we're getting onto the more personal section their role like their function their seniority level things like that since we're doing Cod Outreach you always want to Target decision makers like Founders CEOs owners you name it so that you can do in this section we can actually filter straight up by current job title let's filter for Founders only include Founders and you can see immediately that it fills up with only

Founders in the top right you can see we have 3 Million results for Founders which is a little too broad so you can narrow it down further for example by going into the industry they work in which let's say is Design Services let's include those now we still have over a 100,000 results so narrow it down even further let's say we only want founders of design service companies in Miami Miami there we go and it leaves us with 363 founders of design services companies in Miami this of course is just an example of how narrow you can

actually filter you can leave it a lot more broad you can filter for kind of anything you can imagine as long as the point came across that LinkedIn sales Navigator is an excellent source for finding leads you can essentially find an infinite amount of leads now that we've filtered down our search let's copy the URL and paste it inside of the apify scraper an important option to select is scrape profile SL companies because you want the detailed information otherwise the scraper doesn't extract their email addresses

and since this is an example we only wanted to scrape one page so how do you now get the Json code or turn this into Json code simply you go back up and instead of selecting manual you go to Json now you can see the cookies which was already in Json but also the parameters of the API scrape our turn to Json you can just copy everything and we can go back to n8n so back in n8n we're first going to be setting up the ATP request Noe this note is set to post as you can see what this means is we're sending something we're posting

something because we're triggering the API Scraper on other side kind of and if you remember back to like a second ago when I mentioned three things you'll be needing to set this up properly well first you need the URL endpoint which is in the API documentation you can just copy this because it's like General the only thing that differs between different scrapers is the ID of the scraper which is this and for this particular one this is its ID but if you use a different scraper you would have to change this and you could keep the

rest of the URL which brings me on to the API key you have to like insert your credentials don't worry uh I'm changing my API key after this I just want to show you guys how to set it up so you can follow along from uh beginning to end all we need to do is P the Json inside of this note how you do that is by selecting this option specifying the body as using Json and then pasting it in here one thing to note once you've pasted the code in here you essentially never have to go back to the apify scraper because inside of the code you

can just change the url or change other parameters as you wish so for example this is the URL I just copied from LinkedIn Sals Navigator after I had filtered it down but I can just change out this URL for a new one with new filters and I would never have to go back to apify so it's just a one andone setup essentially so now the HTTP node is completely set up I have my credentials I have the endpoint and I have the Json body with all the parameters which are needed for the scraper to run if I would now press test

step the apy scraper would turn on but I'll showcase that later on in the video now let's say we would have triggered the API scraper and it would start running and after a while it would have scraped the LinkedIn sales Navigator page and it would return us some data which is in this case called a data set you want to retrieve the data set so we want to bring that data inside of our n8n workflow so inside this HTTP node you might notice that there's a little difference we're not sending a body we're not sending any Json code it's not

necessary because we're only retrieving data so we don't need to send anything all we need to retrieve the data is the correct endpoint URL and the credentials in this case are API key now I'll quickly show you test run and kind of explain what happens okay so what happened is that I pressed to test this workflow to do a test run so we sent some signal to the apify scraper and you can see on the right side it worked so it also outputed us a default data set ID please remember this because we're

going to be needing this specific data set ID in the next HTTP note and it triggered the scraper inside of apify so I'll quickly show you what that looks like so here in apify at the top you can see that the scraper is running that's because we triggered it from inside of our n ITN workflow and it's going to take a while meaning that while it is running the data set ID is still empty because it hasn't scraped any data and there's no data to be collected Ed which brings me on to explaining the rest of

the flow let me first quickly dive into the HTTP note set the get which job it is to kind of retrieve the data set so for the person paying close attention you can see that what was grayed out is now green that means it's now valid because there's some output earlier on in the flow so what we did what this output is or what is in green is the data set ID created by triggering the scraper that is also why this is a variable and if you look closely Y and Z matches Y and Z this is a variable because every time you trigger the

scraper a new data set ID is generated and we don't want to manually have to copy and paste each individual data set we want that to be collected automatically and that's why you use this expression for those not familiar with code it looks a lot more difficult that it is how you get this is I can just delete it for you guys and show you you just grab it from the left side paste it in here and you would have the same so you don't have to type this out yourself or do some deep dive internet search to figure out how to get this you

can just pull it from the left side paste it in there and you're done as I said and as I showed you the scraper was still running meaning there is no data set or there's no data to be retrieved in that case if you set it to always output it would output an item but an empty item and then to check later on if there is actually some data at that point in time to be retrieved we have to do a little workaround so first we're using a code I always read in the comments of my YouTube videos that people are a bit scared uh when it comes

to code but once again this is very straightforward all this code does and I wrote it down in here is check if the item it has as input gotten from the previous note is empty if it is empty it outputs not ready if it isn't empty it just outputs all of the items to be passed through but in this case it wasn't ready so the output is not ready then we have set up a if note the if note checks the output from the previous note which was in this case not ready so if the Json status which is once again

just what I took from here and paste it here is not equal to not ready it would go to the true Branch but because it is equal to not ready it goes to the false branch which triggers a weight note which is set to a time interval in this case the time interval I believe 2 minutes so this weight node would actually wait 2 minutes before sending all the information back to the merge Noe which would then trigger the HTTP Noe again to ask it again to retrieve the data the data would then be ready so all of the items would be in here they

would be passed through here it would not output not ready this time but it would just output data so this would also be true instead of false because the output from this would not equal not ready and it would be outputed to our CRM because scraping leads and especially all of the information from those leads takes quite a while I will save both you and me some time and I'll show you what it would look like uh if it would retrieve a data set which is indeed already done finished okay so as I said if the data set was ready it

would fetch all of the information don't know why it's taking a while because it's probably is a lot that would look something like this for this particular scraper so we would have their job title company name profile URL job description etc etc it would pass it on to the code note which would say okay there is actually something so not ready is not needed which would pass it onto the if note which would then say okay not ready is not included in the output which would then output all of the items to

our CRM which would look something like this as I said I use air table as my CRM so inside of the a table which is essentially just like Google Sheets I've set up the table in the following way like first name full name to be uploaded their email address title company name but also their personal LinkedIn URL their company LinkedIn URL and their company website and you can see that everything was present inside of the data so his first name is Michael his personal LinkedIn URL his email address

so we have an address to send an email to we have his company website which we can use to scrape and enrich delete lead and the same goes for his LinkedIn URL we can see that he is the founder of the company so he's actually the decision maker we want to targeting now you have the leads inside of your CRM um you still need to enrich them enriching just means Gathering data on the lead and why would you need that data well you need that data because you want to write a personalized email to them called

Outreach is all about personalization not sending bu emails so with just the email address you have very little you want to do personalized Outreach because that success rate is many many many times higher so what you could do next is just connected to this snippet of a workflow all of these Snippets that you see on screen are inside of the school Community what this part of the workflow does is it takes the uh LinkedIn organization URL as you can see I just dragged it in from the left pasted it

here and it fetches me all of the HTML I'll show you guys quickly so as I said it fetches me the HTML from the LinkedIn organization page it then filters all of that HTML into readable text as you can see the HTML is on the left side there's a lot of code which we don't need and it's also wasted input tokens if you would input it into a large language model we use this code which simply filters out all of the unnecessary code inside of the HTML code and returns us plain text you can see the plain text on

the output side this is all of the text that is on the LinkedIn organization page you can check it out for yourself if you want to verify and what we would do next is we would ask a large language model open AI in this case to analyze all of that data from the LinkedIn page and answer me some questions or write me a summary I'll quickly go over the prompt so you guys get a better understanding so I've asked it to analyze the scraped LinkedIn profile data of a company and then to Output me the company inside so I've asked it to

summarize the company's State Mission and fish you can see that over here I've asked it to Output recent post by the company you can see that here in recent updates and I've asked it to Output the number of employees and like their business model which you can see at the bottom why would you need that information and what can you do with that information well you could plug this onto another large language model which would take all of the output like the entire analysis of the business and write a personalized email opener to be

used in the first gold email you s out to delete which just increases your chances of a reply massively because it makes it look like you've done your research into the company and you actually know what you're talking about little does he know it has all been automated what you could even do is instead of linking this directly to a large language model node to write a personalization you could link it to scrape the company website first gather all that data which also features oftentimes testimonials you could then

attach this to a large language model note which would summarize everything from the company LinkedIn profile and the website to create a very comprehensive picture and another large language model note would read that comprehensive description extract the most relevant topic to write a personalized line about and then plug it onto this to write the personalized line to get even better results I just mean to say that you can plug and play with this however you'd like you can make it as complicated as you want possibilities

are literally Limitless I just wanted to give you guys an idea of what was possible and to show you how I would use this or how I use this in my day-to-day business but all right that is it you now have a fully automated system to extract high quality leads from LinkedIn sales Navigator completely handsfree if you want to download this exact workflow plus every automation I've shared in previous videos and a bunch of exclusive ones I haven't shown anywhere else head over to my school Community I'm

constantly adding new Blueprints advaned workflows and automation strategies so if you're serious about scaling your lead generation you don't want to miss this as I said link is in the description I want to thank you for watching if you found this valuable hit that like button so more people can see it and if you want more automation breakdowns Sales Systems and AI powered workflows make sure to subscribe and turn on notifications so you don't miss the next one drop a comment if you have any questions I try to read and respond

to every single one of them and I'll see you in the next video

