one of the most common questions I get is "How do I actually get a list of highquality leads finding the right companies scraping their data and getting verified contact info." It is a massive time sync if you're doing it manually but what if you could automate the entire process imagine a system that finds companies that fit your criteria pulls in key decision makers and verifies their emails all without you having to lift a finger that's exactly what I'm going to show you today if you are new here I am Clarence and I built

AI powered sales systems that automate lead generation outreach and client acquisition i've spent months refining this process testing different tools and optimizing workflows to make sure every lead that enters my CRM is qualified and ready for outreach the system I'm about to show you isn't just a concept it is the exact automated workflow I use for my own agency to generate leads at scale so by the end of this video you'll know exactly how to set it up for yourself this lead generation system runs in two

parts the first part is about scraping and storing leads from Apollo the second part focuses on qualifying enriching and verifying leads before outreach i've already covered the first part in a previous video so I'll just give you a quick recap here but in this video we'll focus more on the second part where we identify decision makers and verify emails to make sure we're only reaching out to high value prospects if you want to copy this exact system you don't have to build it from scratch inside my

private school community I share all the business SOPs automation workflows and edit end templates that I use to run my own agency including the one I'll be showcasing today so if you're serious about automating your outreach everything you need is inside but but enough about me let's get into the real reason you are here i'm going to walk you through the exact step-by-step workflow that powers the system so you can see how it works and why it is so effective the automation starts when a checkbox is checked in Air Table just

like so this then triggers a web hook sending a record ID to N8N the record ID allows NAN to retrieve the relevant keyword and location for scraping this ensures that every search is targeted and accurate once the trigger fires and it then pulls the keyword and location from air table before sending the request to Apollo the data is URL encoded to handle spaces and special characters properly this prevents errors and ensures the search query is correctly formatted for scraping then after it has been encoded by the LLM

node as you can see United States has a percentage and 20 which means a space in between what we then do is we use a HTTP request node which sends all the information to the Apollo scraper kicking off the lead extraction process this request returns a data set ID which is essential for retrieving the scraped data later however since scraping isn't instant the workflow needs to handle delays to avoid making field requests the system first attempts to retrieve the data set if the scraping isn't complete the output will be empty so we

use a code note to check if the response is empty for the first item you can see that the response was empty on the left side and you can see in that case it is supposed to output not ready and you can see the status output was not ready the if note detects this and triggers a wait note giving the scraper time to finish after waiting the merge node ensures that the data set ID is still available so the workflow can retry retrieving the leads and once the data set is ready the code node allows the leads to pass

through as you can see 26 items were passed through the if nodes didn't detect not ready being present so it passed it to to the true route meaning we get to the remove duplicates note once the data set is successfully retrieved the leads go through a dduplication process to filter out any duplicate companies after that they are added to the lead database in air table each record includes the company name the website LinkedIn URL location estimated employee count and LinkedIn UID this ensures that only unique

highquality leads are passed to into the CRM keeping the data clean and preventing outreach to the same company multiple times to avoid unnecessary rest scraping the final step updates the air table the checkbox that triggered the workflow is unchecked automatically and the current date is stored to track when the keyword was last scraped this keeps everything organized and prevents redundant scraping now that we've scraped and stored our leads in Air Table it is time to qualify categorize and enrich them before moving on to

email verification this part of the workflow ensures that we're only reaching out to relevant companies with key decision makers while filtering out any leads that don't fit our criteria i'll break down each step so you can see exactly how this system works why each note is necessary and how everything flows together the workflow starts when a checkbox is ticked inside the companies table in air table this triggers an automation that sends a record ID to Nitn via a web hook the companies that we just scraped with the

first part of the system were all stored in the companies table i've set up another checkbox over there with another automation that once checked it sends a new trigger this time with a different web hook triggering this one instead of this one by using a web hook the system processes leads on demand rather than in bulk ensuring that only selected companies go through the enrichment process this prevents unnecessary API calls and keeps the system efficient the web hook sends the record ID forward

allowing Nitn to retrieve all relevant company details with the record ID in hand the next step is to fetch the company's details from Air Table this includes all the data we previously scraped and stored in the CRM such as the estimated number of employees the LinkedIn URL and any other relevant information retrieving this data ensures that we know exactly what's already in the CRM before running additional qualification steps more importantly it prevents unnecessary scraping or duplicate processing which saves time

and resources especially doing this at scale even though we already have some company details we need to scrape LinkedIn for three key reasons first we retrieve the about us section which helps us determine if the company is actually a match for our ideal customer profile second we need to extract the exact company name as it appears on their LinkedIn since we will use this for a refer search on sales navigator later in the workflow and lastly sometimes Apollo does not return a company's website since LinkedIn often

includes it scraping it here ensures that we have the correct domain which will be crucial for email discovery later on by pulling all this information now we ensure that we are working with the most complete and accurate data set before even continuing once the LinkedIn page is scraped the response comes back as raw HTML as you can see on the output side which contains a lot of unnecessary data so what we do is we use a code node this node cleans up the response by keeping only the plain text and

filtering out all of the code cleaning the data at this stage ensures that every step that follows works with properly structured and usable information rather than raw and unorganized HTML which will also save us a lot of input tokens for the LMS that we use now that we have a cleanup version of the LinkedIn about a section we pass it to an AI model to determine whether the company is a good fit the AI analyzes the description and determines whether the company aligns with our ICP or ideal customer profile this is

important because not every company scraped is relevant and there is no point in spending resources enriching companies that will never convert by filtering them out early we ensure that only qualified leads continue in the process the output of this step is a simple yes or no which determines whether the workflow will continue or stop this step can also be customized depending on the user specific business needs allowing them to refine the criteria to ensure only the right leads for you are actually going to be

enriched the output from the AI model is passed into an if note which determines what happens next if the AI determines that the company is not a match the if note sends it down the false route stopping enrichment and updating the CRM to mark the lead as disqualified this ensures that we don't waste time or resources on unqualified companies but if the company is a match the workflow proceeds to the next step the decision-making step this decision-m step is essential for maintaining data quality and ensuring that only relevant

companies move forward since we already scraped LinkedIn we now use that data to extract and standardize the company name and website this step ensures that the company name is formatted exactly as it appears on LinkedIn preventing issues when performing referes in LinkedIn Sales Navigator later on if the website URL wasn't available from Apollo we now have it from LinkedIn ensuring that we always have a domain to use for email discovery by standardizing this information we ensure consistency across

all steps in the workflow reducing errors in later API calls with the company qualified the next step is to categorize it based on size using the numbers of employees that we already scraped from Apollo all the way back a simple code note determines whether the company is considered small or large if the company has fewer than 50 employees it is labeled as small if it has 50 or more employees it is labeled as large categorizing companies like this is important because differentiz companies require different outreach strategies

smaller companies might have fewer decision makers while larger companies may require more layers of filtering to find the right contacts by sorting them now we optimize the later steps in the workflow now that the company has been categorized the workflow routes it down the appropriate path the switch note determines whether the company is small or large and sends it through to the corresponding enrichment process smaller companies will follow one path for extracting decision makers while larger

companies take a different route that may involve additional filtering or searching at this point we've successfully qualified the company based on its LinkedIn description extracted and standardized key information like the company name and website and categorized it based on size these steps ensure that only the most relevant leads move forward in the enrichment process setting us up for the next phase scraping sales navigator for decision makers and preparing for email validation now that we've laid this

foundation the next part of the workflow will focus on extracting key decision makers and verifying their contact details before moving them into the CRM so as I already discussed at this point the workflow splits into two branches but the process is the same the only difference is how we scrape sales navigator for small companies so less than 50 employees we scrape all employees since hierarchies are less defined but for larger companies so 50 plus we apply a seniority filter to target decision makers directly saving

time and costs once we have the employees we merge the data and run it through an AI model that filters out only the most relevant decision makers from there we check if any valid contacts were found if not we update the CRM and stop the processing if decision makers are available we move on to the email discovery and verification before finally adding the lead into our CRM since both routes are identical after scraping I'll walk you through the top route which was triggered the first step of this route is triggering the LinkedIn

Sales Navigator scraper this step pulls employee data from sales navigator based on the company size for companies as I said for small companies we scrape all employees since job titles and hierarchies are a lot less defined but for large companies we apply a seniority filter to extract only highle decision makers reducing costs and focusing on relevant context by tailoring the scraping process we ensure that we only gather data that is actually going to be useful for our outreach you can see that

for this company we were able to extract 13 employees 13 items and you can see there is an entire list with all the information you get so their company their job title their location and even their personal LinkedIn URL once these employees are scraped they are merged into a single data set so the 13 items turn into one item using this code note this ensures that all employees retrieved from the sales navigator search are processed together in the next step without this we could end up with fragmented or incomplete data sets

making it harder to identify key decision makers accurately moving on in the next note here an AI model analyzes job titles and filters out only the most relevant decision makers based on predefined roles this model ensures that we only move forward with people who hold executive marketing sales product or compliance roles eliminating unnecessary contacts this step is critical because it prevents outreach to people who have no decision-making power ensuring that we only spend time on high value prospects in this case you can see

that out of the 13 employee scrape only one of them who is the president even was deemed relevant by the AI okay so this is the only one we'll be looking for the email for since we'll be using the output from this LLM later on this workflow with another LLM we need this JSON to be turned into a string so we use a code node to literally turn the JSON into a string this is necessary because when we later match decision makers with verified emails we need this data in a format that can be processed

as part of the CSV to list conversion without this step the system wouldn't be able to process the structured lead information properly before continuing we check whether any decision makers were actually found if none were identified we update the CRM and stop processing to avoid unnecessary email lookups if decision makers are available we proceed to email discovery and verification this simple check using ifnotee ensures that we only continue with leads that have real potential reducing wasted effort now that we have

our filtered decision makers this step sends their details to an email finding service the email finding service is called any mailfinder it's incredibly cheap if you prefer Prospo feel free to use it but it's the same thing the service attempts to find and verify professional email addresses for their selected contacts without this step we would have decision makers but no way to contact them so by automating the process we eliminate the need for manual research saving a lot of time and a lot of effort especially at massive scale

however since email lookup isn't instant especially when doing a bulk search we introduce a short wait period before retrieving results this ensures that the API has enough time to return accurate data preventing errors caused by premature retrieval attempts without this delay we risk retrieving incomplete or incorrect data which would compromise the quality of our outreach and would essentially break this automation workflow once the wait period is over we fetch the CSV file containing the email

results this file includes any emails that were found and their verification status at this stage we still have a mix of verified and unverified emails so the next step is to filter out only the ones we can confidently use this next LLM node extracts only verified emails from the CSV data to ensure accuracy we ignore any email verification status from LinkedIn and only rely on the CSV file to determine which emails are valid if a contact does not have a verified email they are excluded from further

processing this ensures that every email we use is reliable and reduces bounce rate if none of the emails returned would be verified this LLM would output that there are no verified emails found so we use another check note to check the output from the LLM if there were no verified emails found it would update our CRM saying no verified emails but since that's not the case we found one verified email for the one decision maker we're looking for we can proceed uploading all the contact details into

our CRM since a company can have multiple verified decision makers this node splits them into individual record each verified contact is processed separately so that all their information their name email LinkedIn URL and job title is stored properly this step ensures that each lead is correctly formatted before being added to the CRM finally the leads are uploaded to air table this ensures that only highquality leads like decision makers with verified emails are added to the CRM making the outreach highly targeted and effective

every lead that passes through this workflow is fully qualified and ready for immediate outreach what we just walked through is the top route but the bottom route follows the exact same process it just applies a seniority filter for larger companies before scraping in the end both routes achieve the same outcome a fully automated system that scrapes filters qualifies and verifies highv value decision makers ensuring that only the right people with verified emails make it into your CRM this is what makes this system so

powerful instead of wasting time manually searching for leads filtering through job titles and verifying emails everything is handled on autopilot so by the time a lead lands in your CRM you know it's qualified you know it's relevant and you know it's ready for outreach as I mentioned before if you want to skip the trial and error and copy this workflow straight into your own system you can grab it inside my private school community that is where I share all my business SOPs automation workflows and an end templates so you

don't have to build them from scratch you'll find this exact workflow plus other automation systems I personally use to run my agency if you found this helpful drop a like and let me know in the comments what part of this workflow was most valuable for you if you have any questions I try to respond to as many as possible and if you want to see more highlevel automation breakdowns like this make sure to subscribe and turn on notifications so you don't miss any in the future that's going to be it

for this one i want to thank you very much for watching and I'll see you in the next

