gold emails suck when they sound like every other boring robotic message out there but what if you could send emails so personal they feel like you spend hours researching without actually doing any of the work today I'm breaking down how I buil an AI powered system that scrapes websites extracts key insights and writes highly personalized Cote emails on autopilot the same system I've used to lend clients and generate Revenue without any manual Outreach if you're new here my name is Clarence I run a oneman agency that I've skilled to

$80,000 per month using Ai and automation I don't just talk about this stuff I use it every day to lend clients for my own business and help my clients do the same for them the workflow I'll be showing today isn't just Theory it is Battle tested I've personally used it to book meetings close deals and drive revenue on autopilot and in this video I'm going to show you exactly how it works before we dive into this workflow a quick heads up if you want to download this exact automation it is available

inside my school Community along with a ton of other resources on AI Automation and scaling your business you'll get access to my stepbystep video guides n it and blueprints proven business Sops and live Q&A sessions every week so if you're serious about automating client acquisition it is all inside all right that is enough self-promo let's get into the workflow the first thing that kicks off this entire automation is the web hook Noe inside n8n this note is basically the entry point for our system

it just sits there waiting for a signal to start the workflow in this case that signal comes from Air table now to show you exactly how this works I'll jump into my Air table CRM and Trigger the automation every lead in my database has a start enrichment checkbox when I check this box table instantly sends a web hook to NN passing along all the key details of that lead like their name email company website and more this web hook acts as the handshake between air table and NN telling the system hey it's

time to enrich and personalize the lead for outreach so when I click this checkbox and I go back to n it and go to executions we should see the workflow start running yes there you go it's running so the automation from Air table and it then works perfectly the workflow has finished as you can see so after the web hook next comes the air table note which takes the ID that was retrieved from the web hook which was sent by air table you can see it on the bottom left and it uses that record ID to retrieve

everything we need from name email phone number job title company name website in profile industry and locations so now we have all the lead details all the lead information that we had in our CRM in n8n but this raw information alone won't cut it we still need to enrich the lead extract insights and turn it into something actually useful for gold Outreach now that we have all the leads details the next step is to gather more context about their company we start off by scraping the company's homepage to

retrieve its full HTML content you can see all of the HTML on the right side for this demonstration let's take a look at the lead's website lead's website is called making space as you can see the homepage provides a general overview but doesn't really delve into specific details we might need for personalized Outreach however it features a navigation menu with links to Pages like about us for employers for professionals press these sections most likely contain the in-depth information where after by

extracting these links we can identify and Target the most relevant Pages for deeper insights ensuring our Outreach is both informed and tailored let me move back over to n8n for those who need a little extra confirmation if you look here you see the website we scraped is making space and all the text that was on the website but also the navigation links that were in the header or in the footer of the website are somewhere buried inside of this AG HTML now that we've scraped the homepage the next step

is cleaning up the raw HTML to make it usable right now the data is cluttered with code as you can see scripts and unnecessary Elements which isn't useful for us and also isn't useful for the AI to fix this we use a code note that strips out scripts Styles and all HTML Texs leaving behind only plain readable text it also cleans up Extra Spaces and decodes special characters making sure we get a clean output the code you see in the middle of your screen is the code we use to filter the raw HTML to a

cleaned plain text version the code is pretty straightforward it just looks for the symbols you see here and replaces all of them with either a space or with nothing so we turned the raw HTML you can see in the top left into a pretty neatly filtered plain text that you can see here on the right side this filter text won't be used right away but we'll need it later when we extract key insights from the company's website for now it just ensures we're not wasting AI tokens on unnecessary data now that we

have the cleaned up text from the homepage the next step is extracting all the clickable links from the same homepage to do so we use another code note the reason we do this is that the homepage itself usually doesn't contain deep insights we need as I showed you but it does provide links to pages that do like about us case studies maybe a Solutions page or product page if they sell products to extract these links we use a code note that scans the HTML and pulls out every tag which represents a

clickable link the code then formats this into a list of URLs which you can see on the right side at the bottom right actually filtering out any empty or broken links once again the code that we used for filtering the HTML to only extract clickable links is in the middle of your screen for those who aren't familiar with coding this can easily be regenerated using cat GPT or any other llm it looks a lot more difficult than it actually is all you have to ask the llm is to write you a code that takes

raw HTML from a homepage and extracts all urls and it'll give you something very similar to this if not the same as this so now we have a structured list of every clickable page linked on the homepage we can now analyze this entire list of URLs to determine which ones are the most valuable for lead enrichment and which we want to scrape in the future since all of these URLs are outputed as separate items you can see there are 53 separate items the next step is consolidating them into a single structured list right now they're all

separate but to make it easier for the Next Step where we determine which pages are the most valuable we need to merge them into one item so we can use them as a singular input for our llm this ensures that the AI receives a clean organized list of all urls instead of multiple scattered entries with everything combined into one structured input we're now ready to analyze which of these Pages contains the most useful information for need enrichment now we should have a structured list of all the

links from the homepage the next step is figuring out which ones actually matter for personalizing our Outreach not every page is useful things like blogs career Pages pricing and legal Pages don't really help us craft a compelling email to solve this obviously we use an llm to analyze all of the extracted URLs all 53 of them and pick the three most valuable ones for lead enrichment these are typically like about pages containing the company's founding Story Mission and values which help us craft a

natural personalized gold opener could also be things like case studies and success stories because these showcase past clients giving us insights into who they work with and how they solve their problems another one could be use case and solutions these explain what the company actually does and how they help their customers which can help us tailor our Outreach towards them and of course I'm always about full transparency so here is the exact prompt we have given to the llm this prompt and shs it picks

the right Pages as you can see we've defined strict selection criteria telling the AI to prioritize pages that give us deeper insights into the company story clients and their services at the same time we make sure it excludes anything irrelevant like blogs pricing Pages or contact forms having a prompt like this then returns a clean structured list of exactly three urls you can see it picked the about page it picked a Partners page and it picked a press page and sure we always pull the best possible information before moving

forward with this flow now that we have the three most valuable URLs the next step is processing them individually so we can scrape each one separately the reason we need to do this is that the HTTP request note that you can see right here can only handle one URL at a time it can't process multiple pages in a single request so we have to split the one item which is passed by the L into three items which you can see it turns from one item if you look at the llm towards the split node into three items

between the split node and the HTTP request what the split node does is it takes the three URLs and breaks them into separate items so it takes the three in one item into three individual items this means instead of sending one big list of URLs into the HTTP request Noe which wouldn't work each URL is passed through as its own request and showing that every page is scraped properly now that the URLs are split the HTTP request node scrapes each page individually since it can process multiple URLs in one request when I

click the note you can see the raw HTML output on the right side for all three pages it contains everything from the about page it contains everything from the partners page before we can actually use this data we need to clean it up just like we did with the homepage and that is what happens in The Next Step but to make our lives a little easier before we pass it through to a code note we first use an aggregate note what the aggregate note does is it takes the three individual pages and puts them

into a single item once again because right now each page exist as an individual item in NN but to make it easier for us and for the goe node we need to group them together the aggregate node does exactly that it takes the scraped HTML from each page and combines it into a single structured output ensuring that we still know which text belongs to which page now that we have aggregated the scraped HTML you can see the three items that were outputed by the HTTP request node are turned into one item after the aggregate node the

next step is cleaning up the HTML by removing all unnecessary elements script Styles and extra code we use the exact same code as we did for the homepage because it is simple and effective on the left side you can see all of the HTML from three pages and on the right side you can see that only plain text is returned for all three pages as well at this point we now have structured text only versions of the pages ready to be processed in the next step now now that we have clean structured text from the

homepage and three key Pages determined by AI itself it is time to extract actual insights from the website raw text alone isn't very useful we need to categorize it and summarize it into something actionable that we can use for or called Outreach this is where the llm comes in it analyzes all the scraped content and distills the most important details into four key categories it returns us insights about the about page so the company's Mission what they do their founding story it gives us case

studies if they were listed on any of the pages in this case they weren't so outputs not listed it returns us use cases for how they position their service and their product and it returns us product pages so what the client actually sells and how they describe it themselves if any of these aren't found the AI marks them as not listed instead of making assumptions this ensures we get fact-based insights instead of AI hallucinations you can see the final extracted insights already in the output I already went over them a bit it's a

structured summary of everything important about the company and this data isn't just useful for cold emo personalization it is also gold for sales calls imagine jumping on a call and already knowing exactly how the company positions itself what success stories they highlight and how they solve problems without spending any time researching that is exactly the power of using and combining Ai and automation now that we have determined the key insights and we have created a business analysis of this leads company the next

step is cleaning up the company name to use it in our C Outreach and make it more natural sounding sometimes company names include legal terms like LLC in or limited or unnecessary suffixes like and go which doesn't sound natural in a conversation so this llm node refines the name by stripping out the extras and Shing we're left with the version people actually use in day-to-day communication this small change makes our email feel more natural and human instead of looking like they were pulled straight

from a database which would end up in your email getting ignored now that we have the company name cleaned up the next step is one of the most important parts of the entire workflow crafting a personalized C email opener when it comes to called email the first line is everything it is a difference between getting a reply and being ignored a generic opener like I came across your company and want to reach out screams mass email and gets deleted almost instantly but when you start with something specific and relevant to the

company it feels like you actually took the time to research them even though this entire process is automated they'll never know to generate a strong opener we use another large language model that analyzes the about page summary why the about page because it usually contains the company's founding Story Mission and values which are a great conversation starter the AI then crafts a short natural sounding first sentence that references something meaningful about the company looking at the final output

you can see that the a I didn't just generate some generic fluff it pulled real details from the company's own words making the email feel human and intentional let me quickly show you guys The Prompt that generated this output so the prompt is very straight forward as you can see all we told you to do keep it friendly casual and easygoing avoiding anything overly formal or shy and if you don't give it a maximum length you might get an entire paragraph with which sounds robotic very quickly and I've told it the specific output

format I want because we want to upload this personalized line directly into our CRM but also directly into our email provider if the output wouldn't be the exact same Json each and every time we wouldn't be able to copy and paste it or automatically copy and paste it into our email Outreach okay so now that we have all the enriched data the next step is storing it in the CRM and sending the lead to instantly for outreach in the CRM we save key company insights so we upload the summary or the key compy

insights we update the status of the lead because once this flow has completed we know that the lead is enriched and Outreach is on the way we add the clean company name so let's say the scraped lead details included a company name with suffixes or something like Co limited you know what I just talked about we now have a clean version we can use for future reference and we also upload the personalized line uploading the personalized line into your CRM especially when testing out new variations makes it much easier for you

to double check the output you don't have to go into each execution separately inside of NN which will save you a lot of time in the long run believe me on that at the same time the lead is updated in our CRM the lead is also uploaded to instantly or email service software where it is automatically added to an outbound sequence this means the entire process from scraping to outreach happens without any manual input now that the lead has been uploaded I'm switching over to instantly to show you how

everything comes together here you can see that the lead has been successfully added with their contact details the clean company name and the personalization everything we need to start Outreach at this point the lead is fully enriched and ready to go meaning the email sequence can start running automatically with highly personalized messaging at scale and that is it a fully automated AI powered system that scrapes websites extracts key insights and writes highly personalized C emails on autopilot this

workflow isn't just about saving time it's about sending better emails that actually get replies no more generic Outreach no more wasted leads just High converting personalized emails at scale as I already mentioned if you want to download this exact workflow it is available inside my school Community along with step-by-step automation guides andm blueprints proven business Sops and live Q&A sessions every week everything you need to skill your own Outreach and automate client acquisition is inside Community if you found this

valuable hit like subscribe and drop a comment if you have any questions or ideas for the future videos I want to thank you for watching and I'll see you in the next one

