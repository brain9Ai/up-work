hello there my name is Clarence last year I started a b generation agency from scratch and I managed to scale it to 50K per month within just 6 months and that without any prior experience I achieved this by relying entirely on C emo Outreach and of course leveraging Advanced Ai and automation tools whenever I could this helped me handle timec consuming tasks like lead generation email personalization and follow-ups one of the biggest challenges B2B businesses base is a consistent flow of new leads and client acquisition

without a steady flow of qualified leads it's nearly impossible to sustain growth for any business many businesses rely on outdated methods like generic C emails or poorly targeted ads which waste time resources without actually delivering any results on top of all that the competition is ferer than ever making it even harder to stand out and connect with the right prospects to illustrate what not to do and to show you how I've done it let me quickly show you guys two email templates I'll get into the

workflow in a second just let me go over this quickly so on the left we have a very generic email and on the right we have a highly personalized one I'll quickly go over both and then go over the differences so here at the subject line there's no personalization on the right side everything that's within brackets is a variable so you could view that as personalization in some way hi I help companies improve their sales with automation we have a system that gets leads for businesses like yours it's

easy to use and doesn't require much effort on your part lots of companies have used it and seen great results you could too would you like to hop on a call to learn more let me know or book a time here I think it goes without saying that this email on the left wouldn't get very many replies the reason why is because everyone could see this is a bul email bulk send email the time wasn't even taken to insert the first name or the company name anywhere so on the right one of the emails I actually used myself quite a

long time ago May one of the earlier ones in the subject line already I used her company name I also suggest using someone's first name being like first name comma quick question things like that hey first name came across company name on LinkedIn like a little line of personalization I'll go over that in a second I specialize in help in companies like company name connect with your ideal customer profile and scale your sales pipeline by leveraging Ai and automation or done for your lead system

ensures you get bu ready leads without burning your team with additional setup or time commitments here's the kicker so here I introduce a guarantee I always suggest you guys to use a guarantee and then here's the social proof I also use type of company as a variable so in this case let's say they are a grocery store then this would read in the past year we've worked with 200 plus grocery stores just to it would be a lie and please don't lie in your cold Outreach because the customer can easily get

through that in a sales call but just as an example it would demand Authority because people think oh he is already work with 200 different grocery stores he must know he must know what he's doing let me get him on board well I think it goes without saying that you guys could easily notice and Spot the Difference as I said the first email is very generic and could be sent to anyone it's not likely to grab attention or spark any interest the second email on the other hand feels personal and relevant it is clear to the reader that

the sender put effort in understanding the recipient making them more likely to respond but here's the challenge creating personalized emails like the second one takes time up to 30 minutes for just one email to be done manually luckily for both you and I that's where Ai and automation come in with the right tools you can skip this kind of personalization to hundreds or even thousands of emails in mere minutes saving you a lot of time while also dramatically increasing your chances of any success now let me show you the

endit end workflow I personally use for lead enrichment which is just a fancy way of me saying how I gather detailed information about my leads which I can then use to either fill in templates or right personalization normally this process would take hours if done manually but with this workflow flow it is completely automated not only does it save a ton of time but the AI follows precise prompts it eliminate human error ensuring data is accurate and ready to use so what you see on screen are three

individual flows in previous examples or in previous videos excuse me I've put all of them together but in this case I'll just go over I want to go over them one by one to show you what you can get from each source of information and how you can use that information then fill in the templates we'll start off with the trickiest one which is to scrape the personal LinkedIn profile then we'll go on and I'll quickly go over both of these the company LinkedIn profile and the company website scraper because

they're essentially the same you just use a different URL Within These HTTP requests but the code for filtering that HTML then is exactly the same you have to imagine that each of these workflows is being triggered by a web hook for those who aren't aware a web Hook is simply a signal it receives from somewhere well in this case the somewhere is the CRM so a change in my CRM which for a CRM I always use air table but a change within my air table would trigger this flow for those who aren't aware or aren't familiar with air

table I've been talking a lot about it in other videos but I understand that there might be some confusion it's very similar to Google Sheets I'll quickly show you you so we're on the same page so this is one of my air tables this is a giant lead list you can see first name full name their email address and so on and so on and here is the important information like their personal LinkedIn profile URL excuse me they're complete LinkedIn and their company website I have so how I got this lead list is just

from Apollo there's a video on my channel if you're interested it's the best database for getting lead lists so now you know what happens in here so once the change is made inside of the error table let's say I change the status of one to one of my leads to enrich the lead which then triggers an animation which then sends a web hook to start this flow so for the HTTP request node to make a request to the right address in this case I need the information from the lead that's what I do in this note is a very

straightforward setup you just log in with your credentials you set the operation to get well what you can see here is just the name and the table of my air table document and this is a specific individual record idea you can see over here it's set to expression meaning this can also be a variable in most cases let's say you're doing this automatically every lead has their individual record ID so you want this to be an expression this one to be exact if you would paste so replace this one by this one this would just get the

individuals record ID each and every time meaning you don't have to put them in manually and then on the right side we can see the output from triggering this note which is just all the information about the lead well it's the same information that was in the a table I showed a minute ago like their first name company name LinkedIn uro whatever so the distinction between this flow and why it's a lot more elaborate than the other two is because in this case we're we're using an API to call upon a

scraper why do we need to do that well that's because personal LinkedIn profiles are a lot more protected I think it's because they're data sensitive so their privacy is more protected and to kind of circumvent that or what that means before I go about circumvention what that means is that we can't just fetch the HTML because LinkedIn has some built-in code which prevents us from getting all the HTML from the website so we have to use a scraper I use a scraper in apify literally called personal LinkedIn

scraper made by curious coder it's a very straightforward process so for the scraper to work we need two things we need two HTTP nodes to be exact I'll go over the merge note and everything on the right in a second but first let me go over the HTTP node so you know how to set up an API request and to also how to retrieve the data once the scraper is done so first let me dive into this one so for this one it's important that you for this HTTP note it's it's important that you set the method to post so what

happens there is a scraper the scraper is an apify which is just a giant Marketplace for scrapers and we want to activate that scraper with the information that we have in this case with a specific personal profile LinkedIn URL so to do that we post like an action so we send an action to a URL this URL this line over this part of the URL over here this is like the ID of the scraper so this is different for each and every scraper but the rest you can just copy and paste here you fill in the authentic authorization method because

you need to have an API key to access the scraper if you make an apify account you get your own API key so not to worry and then on the page of the apfi actor there is just a bunch of things you can fill in all the instructions over there are very clear I went over it in previous videos so I won't go over it again I think it's a bit redundant because the information is very clear and it's just a copy and paste process the only thing that's of note is that what you see over here in green I pulled

this from the left side so I pulled the LinkedIn URL from here I can show you what this is on the right side so whatever here is in green is once again what's called an expression as said before each lead you prove from the air table is has individual information so you want this to be different each and every time otherwise it wouldn't be automated you would only just scrape the same person each and every time you trigger the flow so I just pulled this from the left from the input side and you can see that this directly this

piece of code actually because it's Json code translate into this so we pull that information which you can then well theost to the scraper was successful as you can see on the right side interesting to note is that it says finished at is null meaning there is it isn't finished yet that's why these merge note these few notes over here were added to this flow I'll go over them in a second but just let me finish it up so once we've sent a post request to the scraper with this note we've like triggered it so we've made it run so now

it's scraping whatever we're asking from it in this case a personal LinkedIn profile from someone but we still need to retrieve that data and to do that we kind of set up the exact same note just a bit easier and we set it to get because we want to get the information so as I just said the method is set to get instead of to post and there is also a different URL what you can see in green once again it's an expression it's from the left side and it's just the default data set ID which is the individual ID of the data set data set

meaning all of the scraped information we wanted it to retrieve so so now you know how to trigger a scraper in apify and how to essentially use any API out there let me go over why there is a merge note and why there is an if note and a weight note that is because if you trigger a scraper it takes a while for it to start running depending on whatever you ask from it it might take a while to scrape all the information so and there's no really no real way of you knowing when it's done so once let's say

this merge note would not be there once this note is completed which takes a split second it would immediately tell the next HP get Noe to get the data set ID or to get the information from the data set but it isn't ready yet most likely the scraper hasn't even started running so there is no information to be passed on to the large language model so we need a workaround in previous videos I used a weight note instead of a merge note and just set it to like a minute so it would wait a minute each and every

time but some people some smart people in the comments on my previous video pointed out that there might be a different way to kind of re-trigger every 10 seconds or whatever to see if the scraper is done and that's what these nodes are for so if there's no output this code says there's no output and does output the not ready then that is forwarded to a if note which just simply checks if the information is ready or not well if there's no information the previous note as I just showed the code note has

output not ready meaning it is sent down the false Branch well what's on the false branch is a weight note which Waits 10 seconds before forwarding all the information again and that's why this merge node is needed because every time a weight Noe is triggered and it forwards like or it outputs anything it doesn't keep the information from the beginning of the flow so we need to store the information from this because we want to keep the default data set idea in this merge node so whenever it's triggered again it'll actually send the

data set idea or I mean excuse me it'll actually attach this data set idea so we can kind of retrieve the data once again so I really hope that made it clear for everyone and this is also a very Nifty trick to check if an API is done or I mean if a scraper is done or whatever API call you've made I don't know if some of you guys are are returning viewers but a few videos ago I made a scraper which scraped Apollo for leads and that was also there was some manual labor involved or manual labor is within

brackets but there was a manual input involved excuse me because I just had to check within apify which shows me if the run is done yes or no before I could continue on but if you would just copy like this part of the workflow and paste it after whichever after any HTTP note which triggers an API this could also be automated so there's no need anymore for you to do any manual checks well we've received the data on the right side you can see the output so this is the output from the data set let me put it to

schema so it's easier for you to see well Nick Mitchell we we know that we can verify that because that's his name in the lead list we can see his description on his LinkedIn profile we can see when he started his job got a lot of information I mean depending on someone's personal LinkedIn profile not everyone is as active and has posted the same thing as everyone else but there are some general Trends n out of 10 times it states where they work so the problem with the output we've received from like the data set so from the

scraper is that all of this is split so as you can see in the Json whatever I don't really know yeah as you can see it's all split it's not one large piece of text so if you would input this into an llm let me show you so you would ask the llm something let's say you ask it to summarize you would have to input drag everything from the left individually which is a lot of hassle certainly because not all information is available always so it would also give you some error codes and it makes it very difficult for the llm to read so

that's why I have using this code note what this code note does is take everything from the left side and put it into one large piece of text for us it's maybe a bit difficult to read but for the llm because it isn't that much information it's quite easy let me show you here you see his first name last name occupation so first name last name occupation it's even in the same order simple piece of code jet gpg generated it gener it for me to be honest and then lastly we go into a large language model

Noe for some reason stuff like that I usually use CET GPT the model I use is mini I mean this is just a showcase example but honestly for things like this like summarizing someone's personal LinkedIn profile or even their company website I use mini because it's more than capable of performing a task like this this is the prompt I've received a lot of questions from people if I could paste a prompt somewhere I've actually decided to start a school Community it'll be done in like a month or so and

I'll just copy and paste all of these flows including the prompts there so please have a bit of patience it'll be there but I suggest using this as a foundation not straight up copying it so the prompt is very straightforward I'll go over it quickly I'm just telling it you'll receive data scraped from a LinkedIn profile use this data to answer the following question concisely and clearly what is the person's current role well you can see their current role on the right side they're a co-founder

how long have they been at their company this is also duration at the company have they had any other roles at the company if yes what were they well they're a Founder so it's very unlikely they've had any other roles because they started the company that have been there since the beginning but let's say you're targeting someone or you're sending C emails to someone who is currently head of HR but started all the way at the bottom of the lab it might be interesting to say something about that about their Career

Development same goes where do they go to school I think it's a bit too personal but maybe if both of you went to the same school it might be a good way of creating a bond and then lastly Write a brief summary this is just based upon the about section of someone this is a brief summary but it does give you or someone from your sales team quite a good overview of who you're talking to and this is good starting point also from for writing and personalization and you can also just just input this as

like data into another llm to write the personalization for you and then lastly I've attached it here so you guys can just follow along is another air table note which would have updated the record so here all the way at the beginning you would get a lead list from Apollo or sales Navigator whatever you would just upload all of those leads into your CRM but they wouldn't be enriched then inside of your CRM you would press a button or change the status anything to trigger to send a a web hook and to

activate this flow then it would go through the flow activate the scraper check if the scraper is done get the information once it's done send it up here summarize or answer questions whatever and then upload all of the answers to the CRM so you can just find it later on which brings us on to the next flow the next flow which is a lot a lot easier both for me to explain and for everyone else to understand so let me just trigger this flow so the flow would have started the same way by a web hook then you once again retrieve all

the information it's the same information because it's the same record ID from the person then we want to scrape the company LinkedIn the company LinkedIn for some reason is always public it's always open so there's never any difficulty retrieving the HTML so I just put the URL in here say get and on the output side you can see it's all the HTML from the page a lot of this like all of the code is unnecessary for us and we of course want to reduce cost so we want to reduce input tokens that's why this code note is here same goes for

this but I'll go over this in a second this code note all it does is remove all of the HTML so it replaces everything with null so you can see how this was all cluttered and difficult to read and then on the right side is the output which is just plain text easy for us to read easy for the llm to read and also a lot less input tokens which makes it cheaper or even cheaper I know some people in on previous videos comments have mentioned deep seek I have been playing around with deep seek it's very

good because this is a flow or these are flows I personally used I also thought it would be best if I just continued using jet GPT because that would be most true to what I know what I know best and what I know performs best for the other one I haven't had enough tries or haven't worked with it long enough to know if the data is consistent and then for the llm I'll quickly go over the script as you can well you can already see the output so I've done the same thing I've just said you will receive

data script from a company's LinkedIn profile and I've asked it to answer some questions like what type of company is it what products do they sell um and in this case like on a company's LinkedIn profile there's also always posts and it's very easy to retrieve those posts so I've asked you to retrieve the three most recent posts think back to the email templates I showed you in the beginning and then especially to the one on the right like the highly personalized one there was within brackets personalization I read through

this quickly oh well here there's a the the new newsletter is live so a thing a p a personal personalized line you could let AI write would be based upon this post which would say hey interesting read in your newsletter blah blah blah I mean it wouldn't be true you didn't read it but it's very easy for you to kind of go back if there's ever sales call to make sure he doesn't figure it out the same thing goes for identify their company type so I use type of company as a variable within the template so you

could just let the AI fill it in based on the information it retrieves from their company LinkedIn last but definitely not not least is I'll trigger this flow and show you that it's exactly the same as the one for the company LinkedIn profile so once again we pull the information from the CRM why do we need this well we need this because we need their company website we need their LinkedIn URL and and we need their company LinkedIn URL then we use the company website here to get all the HTML from the website once again as you can

see just as it did for the LinkedIn page it's all cluttered so this is exactly the same code so both of these flows are identical the only thing that's different is here we're using the URL to LinkedIn company page and here we're just using the URL to the homepage of the company so once again this code note has filtered out all of the unnecessary code and retrieved only plain text and then same as for each and every one of these I've just asked it some questions you can read through it yourself on on

pages there also like testimonials a lot of the times and success stories it might be interesting to mention something about previous successes they've had it's always an ego boost if someone mentions how good you've been doing with someone else so these are testimonials about up which is the name of the company I guess and how good it has been to work with them so yeah I mean you could just say hey congrats on your success with Hub Hub and things like that now that you've seen how this workflow operates let's talk about the

real benefits using Ai and automation to enrich leads takes seconds instead of minutes well instead of maybe an hour making it a game Cher for efficiency this workflow isn't just a standalone solution it is a part of a larger system which you can Implement to either significantly reduce the workload on your sales team or if you're just starting out to lend you your first client with all the data you've gathered AI can seamlessly personalize email templates just like the example I showed earlier helping you stand out and

connect with potential clients effort I want to give you a takeaway to keep in mind is don't rely on AI to ride your and Tire emails from scratch in instead treat AI like an assistant AI is incredibly effective for generating ideas personalizing details or refining specific sections but it still needs your guidance to maintain a consistent tone and structure when crafting your emails focus on building a reusable framework that you can rine and personalize over time this approach would actually allow you to run proper

split tests to figure out what resonates most with your audience for example it would allow you to experiment with subject lines call the action phrasing or the level of personalization in your opening lines what would be my tip is to use AI to handle repetitive tasks like Gathering data generalizing personalization Snippets or analyzing split test results this saves you a lot of time while still giving you control over the creative process so you can continuously improve your Outreach I mean if you're excited start setting up

the system today it's it's a practicable and scalable way to take your cold Outreach to the next level without sacrificing any quality or performance what I've shown you just now is one part of a much larger system for called outbound Outreach if you already have a B2B business and are exploring called email as a new way to acquire clients or if you're looking to improve your existing Outreach I can help there's a link in description below to book a discovery call during call I'll show you exactly how I can build and manage a

fully automated cold Emil Outreach system tailored to your business specifically from crafting the perfect offer to writing personalized Emil copy and handling all the technical setup I'll take care of everything that way you can focus on closing deals and scaling your business I want to thank all of you guys so much for watching if you found this helpful please leave a like it really helps the channel grow if you've got any questions somehow drop them in the comments below I read every single one of them and I'll get back to

you as soon as I can and don't forget to subscribe for more actionable content I'll see you guys in the next one bye

