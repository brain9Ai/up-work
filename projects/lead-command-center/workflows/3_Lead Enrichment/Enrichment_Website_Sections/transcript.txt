do you ever wonder how some businesses book meetings on autopilot while others struggle to even get replies well I build a system that completely changes the game no generic outreach no boring templates just hyper personalized emails at scale fully automated and powered by AI and the best part it works in February alone this system booked 27 meetings for one of my clients i built this workflow for a client who needed a consistent scalable way to generate qualified meetings and the results speak

for themselves 27 meetings in February alone the campaign is still running and since theyve asked me not to share specifics about their offer or emo copy I'm going to respect that what I can show you is the most integral part of the system because I build it from the ground up so the workflow you see on screen what it does is it automates lead research it enriches prospects with AI and personalizes outreach at scale it doesn't just grab a name and slap it into a cold email it pulls real insights

from a company's website how they position themselves their key offerings even proof of success like testimonials or case studies that data is then used to generate highly relevant tailored messages that feel personal even though the whole thing is automated this isn't just a better way to do outreach it is a completely different level of precision and efficiency and in this video I'll walk you through the entire system step by step so you can see how it works if you're new here I am Clarence and I run

a lead generation agency that leverages AI and automation to help businesses skill their outbun sales i built this agency from 0 to $80,000 in monthly revenue in less than a year by focusing on systems that remove guest work and make outreach actually work on top of that I run a private school community where I share all my business SOPs automation workflows and Nitn templates including the exess systems I showcase in my YouTube videos so if you want to copy exactly what I've built and start automating your own outreach you can

find everything inside this community but enough about me let's get into the real reason you're here i'm going to walk you through the exact step bystep workflow that powers this system so you can see how it works and why it is so effective to kick things off this workflow can be triggered in two ways manually or automatically via a webhook normally in a real production setup this would be fully automated so anytime a lead status changes in air table let's say from new lead to ready for outreach

that change would trigger a webhook starting the workflow automatically but for this demo I'm using a manual trigger so I can walk you through every step in detail once triggered the workflow pulls in a lead from air table which I always use as my CRM here's what that would look like what we have is the personal information so their first name full name email and job title we have the company information so the name of the company industryin location website URL we also have the status of the lead as

of now so you can see the status is to do because it hasn't been enriched yet and we also have LinkedIn data so we have their personal LinkedIn URL and their organization LinkedIn URL for this workflow we mainly use the website URL for enrichment the LinkedIn data is available and could be used for fertalization but in this case it is not needed for the email template we are working with now that we have the leads company website we need more information about their business so the first step is scraping the homepage since the

homepage is the only URL we have at this point we sent an HTTP request to retrieve the full HTML content of the page as you can see on the right side the HTML is very messy it includes CSS JavaScript menuse footers and a lot of unnecessary element that is where the next step comes in to clean up the scrape data we use a code note that processes the raw HTML and filters out everything except the actual text content this removes things like styling scripts and navigation elements leaving us with just the meanful text from the

homepage this is important because we only care about the companies messaging their offerings and their positioning not the design of the website now that we have a clean version of the homepage we need to find other important pages on the company's website this is where another code comes in it scans the HTML and extracts all the URLs found on the homepage on the right side of your screen you can see what that output would look like you can see we now have a structured list of links some of them

point to important pages like surfaces pricing or case studies while others are just navigation links the next step is to process these URLs further but before we can do that we need to make one change because right now each extracted URL exists as an individual item in this case you can see there are 143 separate items but to make this data usable for AI analysis we need to merge all these URLs into a single structured list to do that we use another code note which combines all extracted URLs into one

single output which makes it a lot easier for the AI to process now instead of 143 items as you can see on the left side of the code note only one item remains and is needly formatted now that we have a cleaned up list of URLs from the company's website we need to determine which of them actually matter for outreach not every page is going to be useful some are just navigation links legal pages or support sections to solve this we use an AI powered LLM note that analyzes the extracted URLs and categorizes them into four key insights

the first category is company mission which identifies who they are and what they stand for die second is offerings and positioning the third is process and differentiation which determines how they operate and what makes them unique and the last category is proof of success which looks for case studies testimonials or other credibility indicators the LLM processes the list of URLs and selects the best match for each category if no relevant URL exists it simply returns no relevant URL as you can see here after proof of success

instead of making assumptions for this company the AI determined that the mission page is found at visie and missie which is Dutch but would literally be translated to vision and mission the offerings page is located at product/untants the process page is at pro management and for proof of success the AI found that there was no relevant URL with this we now have a curated selection of key company pages that we can analyze further the next step is to actually retrieve and process the full content of these pages as you can see

the AI gave us a single structured item containing four selected URLs but for scraping we need to process each page individually this is where the split out node comes in and after the splitout note there are four separate items because there are four sections or four URLs instead of sending the entire list at once which would break the HTP request we split the URLs into separate items allowing us to scrape them one by one at this point we go from one item which contains a list of URLs to four separate items each containing a single

URL now that we have split the URLs we sent them to the HTP request note once again which scrapes the full HTML content from each of the web pages this works exactly like it did for the homepage scraping we did earlier except this time we're only fetching the specific high value pages identified by the AI as you can see the output of the HTTP request is once again raw HTML which still needs to be cleaned and structured before we can use it since we scraped each page separately and we used a splitout note we now have four

individual items one for each page but to work with this data efficiently we need to merge everything back into a single structured data set this is what the aggregate node does it takes the four separate HTML outputs and combines them into one structured item make it much easier for the AI to process in the next step now that we have scraped the full HTML content from the most relevant pages we need to strip out all unnecessary elements and keep only the plain text this is where the remove HTML

web pages note comes in which is just a simple code note that I gave a nickname when scraping a web page just a refresher the output is often cluttered with HTML CSS and other elements that are not useful for analysis this code what it does it processes the raw HTML and filters it down to just the readable text meaning it removes everything except the actual content leaving behind only the relevant company information this ensures that the AI model does not waste resources parsing through code or

unrelated elements like navigation menus and buttons at this stage we should have a clean data set consisting of only the important text extracted from the selected company pages the next step is to ensure we are not feeding redundant data into the AI which would waste processing power and a lot of tokens after extracting the plain text we need to prevent duplicate content from being sent to the AI in cases where the AI earlier in the workflow was unable to determine a relevant URL for a specific section the

system defaults to scraping the homepage again because it's free anyways this ensures that we always have data available even if a specific subpage is missing however this can lead to unnecessary duplication to handle this efficiently we use the remove duplicate text notes these notes compare the extracted text from the homepage with the text from the scraped section if both contain the same content this node removes one of them ensuring that we do not send identical data to the AI if the texts are different both are retained

and merged into a single structured output this step is particularly important when scaling the workflow across thousands of leads because AI processing costs are based on token usage so removing duplicate content helps reduce token consumption and improve the efficiency of the system by ensuring that only unique and meaningful content is passed forward we make the AI processing leaner faster and more cost effective with the text fully cleaned and optimized the final step is sending it to the AI for analysis instead of

processing everything in a single large request we split the workflow into multiple AI nodes each focusing on a different section this ensures that the AI provides highly structured and consistent outputs through extensive testing I found that sending too much input into a single AI request results in inconsistent and unreliable output the model sometimes overlooks key insight it might misinterpret sections or delivers responses with varying levels of quality to avoid these issues each AI node is designed to focus on one

specific category these categories are the same as we asked the LLM to identify a specific URL for earlier on this workflow so what we have is one LLM note that's supposed to analyze the company and mission section one for the offerings and positioning section one to analyze processes and differentiation and the final one to extract or analyze the proof of success web page each of these AI notes takes two inputs the homepage text and the scraped section text the homepage serves as a summary often containing snippets from various

sections of the site while the section text provides a more indepth view even in cases where a dedicated page was missing like a testimonials page the homepage might still contain relevant information about the proof of success allowing us to still extract meaningful insights using this approach ensures that the AI outputs are clear structured and highly reliable eliminating the inconsistencies that arise when working with large unstructured inputs by keeping each AI request focus on a specific section we guarantee consistent

and high quality results making this workflow scalable for handling thousands of leads now that we have structured and cleaned text input the AI will analyze the company's mission vision and branding using a dedicated LLM note this note is designed to extract only verifiable information while ensuring the output is structured as a cohesive business overview the company mission and fision analysis is highly structured to avoid fabrication or inference dai only includes details that are explicitly mentioned in the provided

text covering several key areas first it identifies the company name and branding including the full name any abbreviations slogans and branding statements then it looks for company history and founders permission statement and core values efficient statement and future goals industry served and target audience services and specialties and then lastly company achievements and recognition so any awards recognitions partnerships or milestones mentioned on either the homepage or on the page of the dedicated

URL that was determined by the LLM to maintain clarity and consistency the AI outputs the analysis in a wellstructured paragraph rather than bullet points there is no conclusion no assumptions and no unnecessary repetition if a particular detail such as the founding year or company awards is missing from the text the AI simply states not mentioned in the text ensuring that no false assumptions or generic placeholders are included by processing each section separately we maximize accuracy and eliminate inconsistencies

ensuring that the extracted insights are fully reliable and can be used for personalization at scale after analyzing the company's mission the AI now focuses on what the company offers and how it positions itself in the market this includes extracting core products and services key differentiators pricing models industry served and target customers the structured prompt ensures that the AI only includes explicitly stated details if pricing is mentioned it is included if not the AI clearly states that pricing information is not

provided separating this analysis from other sections keeps the output structured and uring clarity when using this data for outreach next the AI breaks down how the company operates analyzing methodologies workflows proprietory technology and onboarding processes if the company follows a unique service model such as a consultative approach or a data driven framework the AI captures this the AI also extracts key performance indicators or metrics that the company emphasizes such as efficiency improvements or ROI

once again to emphasize that keeping this separate ensures clear differentiation between what the company offers and how they deliver it making outreach messaging more precise then moving on to the final AI note which focuses on extracting verifiable success indicators such as case studies testimonials notable clients and measurable results if a company has concrete metrics such as revenue increases efficiency gains or cost savs the AI captures them to strengthen credibility additionally it identifies

third party validation including awards recognition or partnerships if no proof of success is mentioned the AI explicitly states it rather than making assumptions this structured approach ensures that outreach messaging highlights the strongest credibility signals making responses much more likely so now that we've gone through the entire workflow you might be wondering what can we actually do with all this data while I can't show you the exact email template used in this campaign I can break down just how

powerful this system is when applied to called outreach the real strength of this workflow is that it goes way beyond basic personalization most outreach just throws a first name and complete name into a generic email and calls it a day but with this system we can craft messages that feel truly personal at scale instead of just addressing the recipient by name we can reference their company positioning by putting the exact way they describe their business we can highlight the specific services they

offer making or pitch more relevant to what they already do if their website includes case studies or testimonials we can acknowledge that success and frame or message in a way that resonates with what has already worked for them and beyond that the system allows us to reference unique differentiators if they emphasize a particular aspect of their surface that sets them apart from competitors we can frame our offer to align with that positioning even more powerful we can identify painpoints based on their website messaging if they

mention a specific challenge they solve for their clients we can craft outreach at speaks directly to that knee making the message feel incredibly relevant and the best part since all of this is extracted and structured automatically we're not just making emails hyper personalized we're making them deeply relevant this is why the system consistently gets high reply rates and booked meetings instead of sending another cookie cutter sales email it feels like we actually did deep research on their company when in reality the

entire thing has been fully automated at this point we've enriched the lead with everything we need to craft a highly targeted email now all that's left is plugging this data into our outrid system and letting automation do the rest and that's the full breakdown of how this system works and why it is so effective if you want to use this exact workflow for yourself you can download it inside my school community where I share all my automation templates business SOPs and inpth guides to help you skill your outreach with AI if this

video gave you value make sure to like subscribe and drop a comment if you have any questions i appreciate you watching and I'll see you in the next

